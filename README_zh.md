# LLaMA CPP Flutter - 本地多模态AI推理

一个展示在安卓设备上使用LLaMA CPP进行本地多模态AI推理的Flutter应用程序。该项目展示了如何在移动设备上直接运行具有图像理解能力的大型语言模型，无需云服务支持。

## 🌟 功能特色

- **本地AI推理**: 直接在安卓设备上运行LLaMA模型
- **多模态支持**: 同时处理文本和图像，实现全面的AI理解
- **文件选择**: 简单易用的界面，用于选择模型文件、多模态投影器和图像
- **自动图像压缩**: 自动将图像压缩至40KB以获得最佳性能
- **实时流式传输**: 实时查看推理结果的生成过程
- **离线功能**: 推理过程无需网络连接
- **隔离处理**: 推理过程中UI不会阻塞

## 📱 应用截图

应用提供直观的界面，包括：
- 显示所选文件的配置面板
- 带有大小信息的图像预览
- 可滚动输出的实时推理结果
- 便于模型和数据管理的文件选择按钮

## 🚀 快速开始

### 环境要求

- Flutter SDK (3.0或更高版本)
- Android Studio 或 带有Flutter扩展的VS Code
- 安卓设备或模拟器 (API level 21+)
- GGUF格式的LLaMA模型文件

### 必需的模型文件

应用正常工作需要以下文件：

1. **主模型**: GGUF格式的LLaMA模型 (如: `ggml-model-Q4_K_M.gguf`)
2. **多模态投影器**: 视觉投影器文件 (如: `mmproj-model-f16.gguf`)
3. **测试图像**: 任何用于测试的图像文件 (如果>40KB会自动压缩)

### 安装步骤

1. **克隆仓库**:
   ```bash
   git clone https://github.com/your-username/llama-cpp-flutter.git
   cd llama-cpp-flutter
   ```

2. **安装依赖**:
   ```bash
   flutter pub get
   ```

3. **构建应用**:
   ```bash
   flutter build apk
   ```

4. **安装到设备**:
   ```bash
   flutter install
   ```

### 模型设置

1. 下载兼容的LLaMA模型：
   - 主模型：任何GGUF格式的LLaMA模型
   - 多模态投影器：兼容的视觉投影器文件

2. 使用应用的文件选择功能选择模型文件
   - 点击"模型"旁的"选择"按钮选择GGUF模型文件
   - 点击"MMProj"旁的"选择"按钮选择多模态投影器
   - 点击"图像"旁的"选择"按钮选择推理用的图像

## 🛠️ 技术架构

### 核心组件

- **主应用**: 单文件Flutter应用程序 (`lib/main.dart`)
- **隔离处理**: 后台推理，防止UI阻塞
- **文件管理**: 动态文件选择和验证
- **图像处理**: 自动压缩和优化

### 关键技术

- **框架**: Flutter 3.x
- **编程语言**: Dart
- **AI库**: llama_cpp_dart
- **图像处理**: image包
- **文件选择**: file_picker包
- **存储**: path_provider包

### 性能优化

- **移动端优化参数**: 为移动设备减少上下文大小和批次大小
- **仅CPU推理**: 针对安卓CPU处理进行优化
- **自动图像压缩**: 保持图像在40KB以下以实现高效处理
- **后台处理**: 使用Dart隔离防止UI冻结

## 📋 使用说明

1. **启动应用**: 在安卓设备上打开应用

2. **选择模型文件**:
   - 点击"模型"旁的"选择"选择GGUF模型文件
   - 点击"MMProj"旁的"选择"选择多模态投影器文件
   - 点击"图像"旁的"选择"选择要分析的图像

3. **运行推理**:
   - 点击"运行推理"按钮
   - 观看AI分析图像并生成描述的过程
   - 结果在可滚动的输出区域实时显示

4. **查看结果**:
   - 滚动查看推理结果
   - 输出显示AI对所选图像的描述

## 🎯 支持的文件格式

- **模型**: `.gguf`格式 (LLaMA CPP兼容)
- **图像**: `.jpg`, `.png`, `.bmp`, `.gif`和其他标准格式
- **自动压缩**: 超过40KB的图像会自动压缩

## 🔧 配置

### 模型参数

应用使用移动端优化参数：
- 上下文大小：1024个token
- 批次大小：128，实现高效处理
- 仅CPU推理，兼容安卓
- 基于质量的图像压缩

### 自定义

您可以在`lib/main.dart`中修改以下内容：
- 模型参数（上下文大小、批次大小）
- 图像压缩设置
- UI布局和样式
- 不同用例的提示格式

## 🐛 故障排除

### 常见问题

1. **文件未找到错误**:
   - 确保使用文件选择器正确选择了模型文件
   - 检查所选文件是否存在且可访问

2. **内存问题**:
   - 使用量化模型（推荐Q4_K_M）
   - 必要时减少上下文大小
   - 确保设备有足够的RAM

3. **推理速度慢**:
   - 使用较小的模型以提高处理速度
   - 降低图像分辨率
   - 检查设备CPU性能

4. **应用崩溃**:
   - 验证模型文件兼容性
   - 检查设备规格
   - 查看错误日志了解具体问题

### 调试模式

启用调试日志的方法：
1. 以调试模式构建：`flutter run --debug`
2. 检查控制台输出以获取详细错误信息
3. 使用`adb logcat`查看安卓特定日志

## 🤝 贡献

我们欢迎贡献！以下是您可以帮助的方式：

1. **Fork仓库**
2. **创建功能分支**: `git checkout -b feature/your-feature`
3. **进行更改**: 遵循现有代码风格
4. **彻底测试**: 确保您的更改在不同设备上正常工作
5. **提交拉取请求**: 清楚地描述您的更改

### 开发指南

- 遵循Flutter最佳实践
- 为复杂逻辑添加注释
- 在多个安卓设备上测试
- 为新功能更新文档
- 尽可能保持向后兼容性

## 📄 许可证

本项目采用MIT许可证 - 详见[LICENSE](LICENSE)文件。

## 🙏 致谢

- **MiniCPM团队与OpenBMB(面壁智能)**: 特别感谢OpenBMB团队在MiniCPM多模态模型方面的卓越贡献以及对开源AI的专注与付出。他们在高效多模态语言模型方面的创新工作使得移动设备上的AI推理成为可能，并为全世界的开发者提供了可访问的解决方案。
- **LLaMA团队**: 出色的语言模型
- **llama.cpp项目**: C++实现
- **llama_cpp_dart**: Dart绑定
- **Flutter团队**: 优秀的移动框架
- **开源社区**: 灵感和支持

## 📞 支持

- **问题**: 在GitHub Issues上报告错误和请求功能
- **讨论**: 在GitHub Discussions中参与讨论
- **文档**: 查看wiki获取详细指南
- **社区**: 与使用此项目的其他开发者联系

## 🔄 更新日志

### 版本 1.0.0
- 初始版本发布
- 基本多模态推理支持
- 文件选择界面
- 自动图像压缩
- 实时流式结果
- 基于隔离的处理

## 🎯 发展路线图

- [ ] iOS支持
- [ ] 多种模型格式支持
- [ ] 批量图像处理
- [ ] 模型下载界面
- [ ] 性能基准测试工具
- [ ] 语音输入支持
- [ ] 导出/导入配置

## 📚 资源

- [LLaMA官方页面](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [Flutter文档](https://flutter.dev/docs)
- [Dart语言指南](https://dart.dev/guides)

---

为AI和Flutter社区用❤️制作